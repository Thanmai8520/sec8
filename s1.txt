My update of dashbaord
Dashboard Strategy Review
	•	Presented strategy document in Confluence with 5 proposed metrics
	•	New metric: “history records process” under PKYC namespace
	•	Namespace concept explained as container/boundary for metrics
	◦	Similar to surname - prevents naming conflicts
	◦	Same metric name allowed under different namespaces
	◦	Error occurs if duplicate metric created in same namespace
Proposed Metrics & Implementation
	•	5 custom metrics to be generated from code (not available from existing infra)
	•	Dashboard layout discussion:
	◦	Overview section: invocation, success rate, error rate for all lambdas combined
	◦	Concern about visual clutter with multiple lambda lines
	◦	Need R&D to determine optimal approach (metrics vs logs)
	•	Design principle: Dashboard for non-technical users
	◦	Should show overall application health at a glance
	◦	Focus on processed records vs cases created for health check
	◦	Error tracking and tallying differences
Technical Guidance
	•	Start building dashboard infrastructure without interfering with code
	•	Can implement infra components now
	•	Hold off on creating custom metrics until after show and tell
	•	Consider user perspective: non-technical stakeholders need simple health overview
	•	Iterate and refine based on feedback
Scheduler Configuration
	•	Current setup: 55-minute self-triggering lambda with cron for date control
	•	Triggers only on 13th of each month, stops on 14th
	•	For demo: Change to 5-minute intervals for show and tell
	•	Make configuration change on day 14, then revert after demo
Next Steps
	•	Begin dashboard development (infrastructure components)
	•	Research optimal display method for multi-lambda metrics
	•	Configure scheduler for 5-minute demo intervals
	•	Wait for approval before creating custom metrics
	•	Prepare for iterative refinement process





Creating custom metrics in Cloud Watch for AWS Lambda
CloudWatch Dashboard Setup
	•	Access CloudWatch → Dashboard section to create custom dashboards
	•	Current available metrics viewable under “All metrics” → shows all services generating metrics
	•	Lambda functions (starting with “kyc”) available for monitoring
	◦	Built-in AWS metrics show invocations, duration, errors
	◦	Infra metrics generated automatically by AWS
	◦	Limited insight into application-level success/failure rates
Custom Metrics Strategy
	•	Built-in invocation metrics insufficient for detailed success/failure tracking
	•	Need custom application metrics for granular monitoring:
	1	S3 record count - track files read successfully
	2	TT event publish successful - count successful event publications
	3	TT publish failed - track publication failures
	4	Salesforce case creation success/failure rates
	•	Publish custom metrics from Lambda code at key processing points
	•	Enables precise tracking without complex log parsing
Dashboard Widget Configuration
	•	Statistics selection critical: sum, maximum, minimum, average, percentile
	◦	Duration metrics: use max/min/percentile for performance analysis
	◦	Count metrics: use sum for totals
	•	Time range selection affects data granularity
	•	Widget types: metric widgets for numbers, log widgets for error details
Error Logging Requirements
	•	Standardize error message format across all Lambda functions
	•	Required error log structure:
	◦	Log level: ERROR (for filtering)
	◦	Transaction/Correlation ID
	◦	Error message
	◦	Error code (if available)
	•	All team members must align on consistent logging format
	•	Enables log widget queries to display specific failure details
Next Steps
	•	Thanmai: Create documentation by tomorrow showing implementation plan
	◦	Detail where changes needed across system
	◦	Specify custom metrics to implement
	◦	Define error logging format
	•	11:00am tomorrow: Review documentation and finalize approach
	•	Begin development after plan approval


Dashboard should have: tally up the info how many publish as an event, how many cases sucess for tt events, failure for reason, 
1. S3 how many
2. Sales forces cases how many -failed and pass metrics


aws lambda update-function-code \
--function-name bbkyc-tms-token-manager-dev-lambda \
--s3-bucket bbkyc-lambda-code-dev-bucket \
--s3-key bbkyc-tms-token-manager-lambda-new.zip \
--region eu-west-2

aws lambda invoke \
--function-name bbkyc-tms-token-manager-dev-lambda \
--payload '{}' \
--region eu-west-2 \
response.json

cat response.json
PKYC-INFRA/
│
├── app-iam-roles/
│   ├── metadata.yaml
│   ├── product.template.yaml
│   └── README.md
│
├── dynamo-dbs/
│   └── product.template.yaml
│
├── kinesis-streams/
│   └── product.template.yaml
│
├── s3-buckets/
│   ├── op/
│   │   └── metadata.yaml
│   └── tm/
│       └── product.template.yaml
│
├── G015/
│   ├── README.md
│   └── tms-token-manager-lambda/
│       ├── iam-roles.yaml
│       ├── README.md
│       └── modules/
│
└── TracFi/



PROJECT1
│
├── bb-pkyc-ttbdpprocessor
│   ├── .venv
│   ├── bdp-consumer-lambda
│   ├── case-creation-lambda
│   ├── cft
│   │   ├── app-iam-roles
│   │   ├── dynamo-dbs
│   │   ├── kinesis-streams
│   │   ├── lambdas
│   │   ├── s3-buckets
│   │   ├── sqs
│   │   ├── iam-roles.yaml
│   │   └── sample-cft.yaml
│   │
│   ├── cta-rules-lambda
│   ├── shared
│   ├── .gitignore
│   ├── bbkyc-cta-rule-lambda.zip
│   ├── metadata.yaml
│   ├── product.template.yaml
│   ├── README.md
│   └── setup.sh

AWS Lambda: bbkyc-tms-token-manager

🔹 Purpose

Manages the TMS token lifecycle —
fetches, validates, and updates the token used by downstream services (like bbkyc-sf-case-creator).

⸻

🔹 Trigger
	•	Source: AWS EventBridge
	•	Rule Name: bbkyc-tms-scheduler
	•	Schedule: Every 55 minutes
	•	Active Window: Between 12 AM and 3 AM (processor days)

⸻

🔹 Execution Flow

1️⃣ Trigger

AWS EventBridge invokes the Lambda (bbkyc-tms-token-manager) every 55 minutes.

2️⃣ Token Fetch & Validation
	•	The Lambda fetches the current TMS token from:
	•	AWS Secrets Manager
	•	Secret Name: bbkyc-tms-token

3️⃣ Validate Token
	•	Calls Border Control API endpoint (external REST API)
	•	Purpose: Check if the stored TMS token is still valid.
	•	API Example: https://bordercontrol.example.com/validate
	•	If token is valid → end process (no change).
	•	If token is invalid / expired → immediately fetch a new token.

4️⃣ Fetch New Token
	•	Calls another Border Control API endpoint:
	•	API Example: https://bordercontrol.example.com/token
	•	Uses credentials or payload (e.g., username/password or API key)
	•	Receives a new TMS token in API response.

5️⃣ Update Token in Secrets Manager
	•	Overwrites the old secret with new JSON:





npm config set proxy http://primary-proxy.gslb.intranet.barcapint.com:8080
npm config set https-proxy http://primary-proxy.gslb.intranet.barcapint.com:8080
npm config set strict-ssl false


npm config list
curl -I https://registry.npmjs.org

sudo xcode-select -s /Applications/Xcode.app/Contents/Developer
sudo xcodebuild -license
xcode-select --install
xcode-select -p


export DEVELOPER_DIR=/Applications/Xcode.app/Contents/Developer
export OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES

echo 'export DEVELOPER_DIR=/Applications/Xcode.app/Contents/Developer' >> ~/.zshrc
echo 'export OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES' >> ~/.zshrc
source ~/.zshrc


Hi Sumanta,
I was referred to you by Sai Krishna regarding creating a DevOps strategy for our Lambda functions to flow from GitLab to AWS. I’d like to discuss the current approach and understand what’s going on with it. Please let me know a convenient time to connect.


Just to clarify our flow — after the code is committed to GitLab, do we have a CI/CD pipeline that publishes artifacts to Nexus? And from Nexus, is there another pipeline that uploads them to S3 for Lambda deployments?


import boto3
import base64
import json
import os
from datetime import datetime

# Kinesis client
kinesis = boto3.client("kinesis")

# Destination stream from environment variable
CTA_EVENT_STREAM = os.environ["CTA_EVENT_STREAM"]

def lambda_handler(event, context):
    records_to_put = []

    for record in event["Records"]:
        # Decode the incoming TT event
        payload = base64.b64decode(record["kinesis"]["data"]).decode("utf-8")
        event_data = json.loads(payload)

        # --- Apply CTA Rule Processing (example rule) ---
        cta_event = {
            "ttEventId": event_data.get("id"),
            "status": "CTA_READY" if event_data.get("priority") == "HIGH" else "CTA_PENDING",
            "timestamp": datetime.utcnow().isoformat(),
            "originalEvent": event_data
        }

        # Prepare record for CTA event stream
        records_to_put.append({
            "Data": json.dumps(cta_event).encode("utf-8"),
            "PartitionKey": record["kinesis"]["partitionKey"]
        })

    # Push batch to CTA event stream
    if records_to_put:
        response = kinesis.put_records(
            StreamName=CTA_EVENT_STREAM,
            Records=records_to_put
        )
        print("PutRecords response:", response)

    return {
        "statusCode": 200,
        "body": f"Processed {len(records_to_put)} TT events into CTA events"
    }