### AWS Deployment Migration

-   Move application from EU West 2 (London) to EU West 1 (Ireland)
    -   Not the repo/code, just the deployment
    -   Lambda currently running in EU West 2
    -   Use same deployment scripts but change region to EU West 1

### CSM Tool Integration Research

-   CSM tool already installed and available in EU West 1
    -   Pre-installed in AWS with various components
    -   May include ACM (Certificate Manager), Secret Manager, Lambda, event triggers
-   Research requirements:
    -   Identify all CSM tool components
    -   Determine input requirements to fetch details from target safe
    -   Understand CSM vault/store functionality
    -   Document what details need to be sent to CSM tool
-   Background work for next sprint planning


FUNCTIONAL DASHBOARD (Business View)

Widget 1 â€” Total Events Processed Today

Source:
	â€¢	Lambda: bbkyc-cta-event-producer
Metric: Custom metric: TotalEvents (if you send) or use Kinesis IncomingRecords
Namespace: PKYC/Salesforce or default Kinesis

â¸»

Widget 2 â€” Salesforce Case Creation Success / Fail

Source:
	â€¢	Lambda: bbkyc-cta-case-creator
Metrics:
	â€¢	SFCaseCreationSuccess (custom)
	â€¢	SFCaseCreationFailure (custom)
Namespace: PKYC/Salesforce

â¸»

Widget 3 â€” End-to-End Latency (Business Latency)

Source:
	â€¢	Lambda: bbkyc-cta-case-creator
Metric:
	â€¢	EndToEndLatency (custom metric you create)

â¸»

Widget 4 â€” Token Refresh Success / Failure

Source:
	â€¢	Lambda: bbkyc-tlaa-token-manager
Metrics:
	â€¢	TokenRefreshSuccess
	â€¢	TokenRefreshFailure
(You already log token refresh in CloudWatch; just publish 2 metrics)

â¸»

Widget 5 â€” Audit Summary (Audit Events Written)

Source:
	â€¢	Lambda: bbpyc-audit-processor
Metric:
	â€¢	AuditEventWritten (custom)
	â€¢	OR Count of DynamoDB writes (Dynamo metric: SuccessfulRequestLatency)

â¸»

Widget 6 â€” Failure Summary (Combined)

Combine failures from:
	â€¢	Glue
	â€¢	Lambda
	â€¢	SF Case creation
	â€¢	Token
You combine these metrics using math expression or by adding them in one widget.

Sources:
	â€¢	Glue: Failed
	â€¢	Lambda: Errors
	â€¢	Case Creator: custom SFCaseCreationFailure
	â€¢	Token: TokenRefreshFailure

â¸»

Widget 7 â€” Daily Retry Success Rate

Source:
	â€¢	Lambda: bbkyc-cta-case-creator
Metric:
	â€¢	RetrySuccess (custom metric)

â¸»

ğŸŸ£ Functional Dashboard = ONLY these functions

bbkyc-cta-event-producer â†’ Total events
bbkyc-cta-case-creator â†’ Case creation + E2E Latency + Retry
bbkyc-tlaa-token-manager â†’ Token Success/Fail
bbpyc-audit-processor â†’ Audit Summary
Glue Jobs â†’ Only failure count (combined)

â¸»

ğŸ”µ 2. TECHNICAL DASHBOARD (Engineering View)

Now the technical dashboard will ONLY contain component-level metrics, optimized & reduced.

â¸»

Widget 1 â€” Combined Lambda Errors

Source: All Lambdas
	â€¢	bbkyc-cta-event-producer
	â€¢	bbkyc-cta-case-creator
	â€¢	bbpyc-audit-processor
	â€¢	bbkyc-cnc-bdp-creator
	â€¢	bbkyc-tlaa-token-manager

Metric: Errors (built-in)
Combine all 5 Lambdas in 1 widget

â¸»

Widget 2 â€” Combined Lambda Duration (Latency)

Source: Same 5 Lambdas
Metric: Duration

â¸»

Widget 3 â€” Kinesis Stream Health

Stream Names:
	â€¢	bbkyc-cta-event-stream
	â€¢	bbkyc-et-event-stream

Metrics:
	â€¢	IteratorAgeMilliseconds
	â€¢	ReadProvisionedThroughputExceeded

This widget tells if Lambda is lagging.

â¸»

Widget 4 â€” Glue Job Failures

Jobs:
	â€¢	bbkyc-et-bdp-glue-job
	â€¢	bbkyc-et-event-producer-job
Metric: Failed

â¸»

Widget 5 â€” Glue Job Runtime

Metric: Glue â†’ glue.job.RunTime

â¸»

Widget 6 â€” SQS Queue Depth

Queue: bb-pkyc-audit-event-queue
Metrics:
	â€¢	ApproximateNumberOfMessagesVisible
	â€¢	ApproximateAgeOfOldestMessage

â¸»

Widget 7 â€” DynamoDB Throttles

Table: bbkyc-audit-data
Metrics:
	â€¢	ReadThrottleEvents
	â€¢	WriteThrottleEvents

â¸»

Widget 8 â€” EventBridge Invocation Failures

Rule names:
	â€¢	bbkyc-et-bdp-scheduler
	â€¢	bbkyc-cta-scheduler

Metric:
	â€¢	FailedInvocations

â¸»

Widget 9 â€” Batch Job Failure + Duration

Job: bbkyc-batch-processor
Metrics:
	â€¢	JobFailed
	â€¢	JobRunningTime

â¸»

Widget 10 â€” CloudWatch Alarms Summary

Shows how many alarms:
	â€¢	In ALARM state
	â€¢	In OK state

â¸»

ğŸ”¥ FINAL SUMMARY (Copy/Paste)

âœ… Functional Dashboard
	1.	Events Processed â€” bbkyc-cta-event-producer
	2.	SF Case Success/Fail â€” bbkyc-cta-case-creator
	3.	E2E Latency â€” bbkyc-cta-case-creator
	4.	Token Refresh Status â€” bbkyc-tlaa-token-manager
	5.	Audit Summary â€” bbpyc-audit-processor
	6.	Failure Summary (combined glue+lambda+case+token)
	7.	Retry Success â€” bbkyc-cta-case-creator

â¸»

ğŸ”µ Technical Dashboard
	1.	Lambda Errors (ALL lambdas combined)
	2.	Lambda Duration (ALL lambdas combined)
	3.	Kinesis IteratorAge (CTA + Event stream)
	4.	Glue Failures
	5.	Glue Runtime
	6.	SQS Queue Depth (audit queue)
	7.	DynamoDB Throttles (audit table)
	8.	EventBridge Failure
	9.	Batch Job Failure + Duration
	10.	CloudWatch Alarms Summary
Functional Dashboard (Business View)
	1.	Total Events Processed
	2.	Case Creation (Success/Fail)
	3.	End-to-end pipeline flow status
	4.	Token Refresh Status
	5.	Audit Summary
	6.	SLA / Average Latency
	7.	Daily Failures Summary (combined)
	8.	Retry Success Rate

â¸»

Technical Dashboard (Engineering View)
	1.	Combined Lambda Errors/Throttles
	2.	Combined Glue Job Failures
	3.	Kinesis Stream Health (iterator age + PutRecordFailures)
	4.	SQS Queue Depth + Oldest Message Age
	5.	DynamoDB Throttles
	6.	Token Refresh Lambda Failure Details
	7.	EventBridge Invocation Failures
	8.	Batch Job Status
	9.	CloudWatch Alarms Summary
	10.	Technical End-to-End latency


Glue uses Spark under the hood, and Spark publishes its own metrics.
CloudWatch marks them as â€˜customâ€™ only because the namespace is non-AWS, but they are AWS-generated and cannot be moved to AWS/Glue.
Default AWS/Glue metrics are very high level; the detailed metrics exist only in the custom namespace, so we cannot remove them.

Hereâ€™s the reason why your Glue job metrics appear under â€œCustom Namespacesâ€ even though they are AWS-managed ğŸ‘‡

â¸»

âœ… Why Glue Metrics Look Like Custom Metrics

AWS Glue publishes two types of metrics into CloudWatch:

1. AWS-Provided Default Glue Metrics

These are automatically created under the namespace:
Examples:
	â€¢	Glue.JobRunTime
	â€¢	Glue.DriverMemory
	â€¢	Glue.SparkJobs
	â€¢	Glue.SparkErrors

These are standard AWS metrics.

â¸»

2. Spark / JVM / Executor Level Metrics

These come from the Glue driver & executor, such as:
	â€¢	glue.1.jvm.heap.usage
	â€¢	glue.driver.system.cpuSystemLoad
	â€¢	glue.driver.aggregate.numCompletedStages
	â€¢	glue.driver.filesystem.write_bytes
	â€¢	glue.driver.aggregate.elapsedTime
	â€¢	etc.

These metrics come from Spark internal monitoring system used by Glue.
AWS publishes them to CloudWatch automatically, but they do NOT belong to AWS/Glue namespace.

ğŸ‘‰ They are published under a namespace that CloudWatch labels as â€œCustomâ€
â€¦but they are not created by YOU â€” AWS publishes them.

So CloudWatch calls them â€œcustomâ€ only because the namespace is not AWS/*.

â¸»

ğŸ§  Why AWS Glue Puts Spark/JVM Metrics in Custom Namespace

Because:

âœ” Spark/Glue job internals â‰  Standard AWS metrics

Glue uses Apache Spark and sends Spark driver/executor metrics using a non-AWS namespace.

âœ” AWS/Glue namespace is only for high-level metrics

Detailed metrics (JVM heap, CPU load, tasks, SQL stages, spills, shuffle bytes, etc.) are too specific for standard AWS namespaces.

âœ” Glue auto-publishes them, but CloudWatch categorizes by namespace, not source

Anything not starting with AWS/ goes into â€œCustom Metricsâ€.




process in breif

step1

fetch Creds foom CSM
Step2
Fetch certificate from certs folder

Stepâ‘¢fetch TIAA token inexchage of certâ‘¡ & cred of (cSM) i.e., step 1

â‘£ Save token inside Secret

Reusable TIAA token manager



(Acume its a tool)
SC Product â€“ TIAA Token Manager
	â€¢	AWS Event Scheduler
	â€¢	Lambda
	â€¢	IAM Roles: Access
	â€¢	â†’ Lambda â†’ CSM
	â€¢	â†’ Lambda Event Bridge
	â€¢	â†’ Secret

â¸»

Configurable Properties
	â€¢	Prefix / Domain
	â€¢	Env
	â€¢	Secret Name
	â€¢	Cert Path (ACM) / File Path
	â€¢	Auth Role
	â€¢	Secret URL
	â€¢	TIAA Auth URL
	â€¢	CSM Lambda Name



bbkyc-tt-event-producer-job
â”‚
â”œâ”€â”€ assets
â”‚
â”œâ”€â”€ src
â”‚   â””â”€â”€ main
â”‚       â”œâ”€â”€ helpers
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ audit_helper.py
â”‚       â”‚   â”œâ”€â”€ kinesis_helper.py
â”‚       â”‚   â””â”€â”€ utils.py
â”‚       â”‚
â”‚       â”œâ”€â”€ bbkyc_ttdbp_glue_job.py
â”‚       â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ test
â”‚   â”œâ”€â”€ helpers
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ audit_helper_test.py
â”‚   â”‚   â”œâ”€â”€ kinesis_helper_test.py
â”‚   â”‚   â””â”€â”€ utils_test.py
â”‚   â”‚
â”‚   â”œâ”€â”€ bbkyc_ttdbp_glue_job_test.py
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ conftest.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .flake8
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .gitlab-ci.yml
â”œâ”€â”€ commands.txt
â”œâ”€â”€ data-producer.py
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ pytest.ini
â”œâ”€â”€ README.md
â””â”€â”€ template.yaml


Dashboard metrics enhancement for glue job audit
Dashboard Metrics Implementation
	â€¢	Current dashboard matrices added partially, need completion for glue job monitoring
	â€¢	Required metrics for glue job:
	1	Read count from iceberg table
	2	Processed count from read data
	3	Success count from processed data
	4	Failure count from processed data
	â€¢	Additional metrics needed: job run success/failure counts
	â€¢	Exception handling matrices missing, need to be added for failure conditions
Dashboard Visualization Requirements
	â€¢	Need visual appeal beyond just numbers display
	â€¢	Required chart types:
	â—¦	Pie charts for pass/fail/total comparisons
	â—¦	Bar graphs for trend analysis
	â—¦	Failure graphs for monitoring
	â€¢	Make data more consumable for large numbers (e.g., 132,516 records with 10,500 passes)
	â€¢	All widgets must be moved to CFD (currently some manually added)
Service Priorities
	â€¢	First priority services (in order):
	1	Glue job audit Lambda
	2	Kinesis
	3	Kinesis SQS
	4	DynamoDB
	â€¢	Secondary priority: CTA case creation, TV dashboard bits
	â€¢	Custom alarms can be added for these five services first
Code Organization Standards
	â€¢	Keep metrics code separate from main business logic
	â€¢	Create utility functions for CloudWatch metrics creation
	â€¢	Use component name + environment for namespaces (not pkyc/salesforce format)
	â€¢	Add environment as dimension in metrics
	â€¢	No commits without verification
	â€¢	Package only helper folder for glue job, not job script
Action Items
	â€¢	Speaker B: Implement missing glue job metrics and exception handling
	â€¢	Speaker B: Create clean code structure with separate metrics utilities
	â€¢	Speaker B: Work on dashboard visualization comparisons after metrics completion
	â€¢	Speaker C: Provide guidance on dashboard visualization approaches after initial exploration




âœ… TIAA Token Lambda â€“ Simple Steps (No Code)

1ï¸âƒ£ EventBridge triggers your Lambda
	â€¢	It runs automatically every 55 minutes.
	â€¢	You donâ€™t schedule anything inside the Python code.

â¸»

2ï¸âƒ£ Lambda sends a request to TIAA to get a new token
	â€¢	The Lambda calls the TIAA token API.
	â€¢	TIAA gives back a fresh access token.

â¸»

3ï¸âƒ£ Lambda saves the token into AWS Secrets Manager
	â€¢	The token is stored in the secret called
bbkyc-tiaa-token.
	â€¢	This keeps the token safe and encrypted.

â¸»

4ï¸âƒ£ Lambda updates the token every time it runs
	â€¢	Every 55 minutes, the old token is replaced with a new one.

â¸»

5ï¸âƒ£ Other Lambdas read the token from Secrets Manager
	â€¢	When the case-creator or other services need the token,
they read it from Secrets Manager instead of creating it themselves.

â¸»

6ï¸âƒ£ CloudWatch logs show the status
	â€¢	You can see in logs whether token fetch was successful.

â¸»

â­ One-line summary for presentation

â€œOur TIAA Token Lambda refreshes the token every 55 minutes using EventBridge and stores it in Secrets Manager so other Lambdas can use it safely


bb-tiaa-token-manager
â””â”€â”€ main
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ lambda_handler.py
    â”‚
    â”œâ”€â”€ helpers
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ api_client.py
    â”‚   â”œâ”€â”€ scheduling_helper.py
    â”‚   â”œâ”€â”€ secrets_helper.py
    â”‚   â””â”€â”€ utils.py
    â”‚
    â”œâ”€â”€ test
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ lambda_handler.py
    â”‚   â”‚
    â”‚   â””â”€â”€ helpers
    â”‚       â”œâ”€â”€ __init__.py
    â”‚       â””â”€â”€ utils.py
    â”‚
    â”œâ”€â”€ env-vars.json
    â”œâ”€â”€ README.md
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ response.json
    â””â”€â”€ update-lambda-stack.sh

Configuring production token management
Production Configuration Strategy
	â€¢	Token URL must be configurable for different environments
	â—¦	Sit/UAT/E2E environments use same URL
	â—¦	Production requires completely different token endpoint
	â€¢	Implementation approach: property files for environment-specific values
	â—¦	Avoid hardcoding production URLs in code
	â—¦	Enable configuration override at deployment time
Certificate Management Architecture
	â€¢	Current GPT project packages certificates with code
	â—¦	Creates maintenance burden when certificates expire
	â—¦	Requires full redeployment for certificate updates
	â€¢	Proposed solution: AWS Certificate Manager integration
	â—¦	Externalize certificates from application code
	â—¦	Enable certificate updates without code redeployment
	â—¦	Reduce operational complexity for certificate lifecycle
	â€¢	Certificate requirements for external API calls
	â—¦	Need public key for target system authentication
	â—¦	Target system validates using private key
Code Reusability Goals
	â€¢	Creating reusable product across domains
	â—¦	Same codebase for GPT and future 50 KYC projects
	â—¦	Configuration-driven deployment approach
	â€¢	Configurable components identified:
	â—¦	Token URL endpoints
	â—¦	Certificate paths and keys
	â—¦	Username/password via secret manager
Technical Implementation Details
	â€¢	Secret manager already handling username/password
	â€¢	Certificate paths currently read from local directory
	â€¢	Will transition to AWS Certificate Manager once research complete
	â€¢	Temporary approach: keep certificates with code until better solution implemented
Next Steps
	â€¢	Speaker B: Study provided GPT code to understand current implementation
	â€¢	Thanmai kora: Research AWS Certificate Manager integration options
	â€¢	Wednesday: Face-to-face team meeting
	â—¦	Include full team + J
	â—¦	Explain Certificate Manager approach
	â—¦	Discuss TIA token integration details


The purpose of this dashboard is to monitor and visualize the performance and reliability of our BBKYC data processing workflow.
It helps us understand how effectively our Lambda functions are processing data, publishing to downstream systems, and creating Salesforce cases.â€

â¸»

ğŸ§© 2. Give a Brief Architecture Context

â€œOur workflow starts from S3 and EventBridge triggers, and then several Lambda functions handle event publishing, Salesforce case creation, and auditing.
Each Lambda function pushes success/failure metrics to CloudWatch, and those metrics are aggregated here in the dashboard.â€

(If needed, show the architecture diagram you shared earlier to link where each metric comes from.)

â¸»

ğŸ“Š 3. Explain Each Metric Tile Clearly

ğŸŸ¢ S3RecordsProcessed (6)

â€œThis shows the number of records successfully read from the S3 bucket by the Glue job and processed downstream.
It helps ensure the data ingestion layer is functioning.â€

â¸»

ğŸŸ¢ TTPublishSuccess (25)

â€œThis represents the successful transmissions from the Lambda (bbkyc-tt-event-producer) to the TMS system via the Kinesis stream.
It indicates that events are being published and consumed correctly.â€

â¸»

ğŸ”´ TTPublishFailure (â€“)

â€œWe havenâ€™t seen any publish failures in this period, which means no event drop occurred.
If this value rises, it could indicate a downstream service issue or timeout.â€

â¸»

ğŸŸ¢ SFCaseCreationSuccess (37)

â€œThis metric is from the Salesforce case creation Lambda (bbkyc-sf-case-creator).
It tracks the number of cases successfully created in Salesforce after the TMS token validation and event processing.â€

â¸»

âš« SFCaseCreationFailure (0)

â€œCurrently, there are no failures reported in Salesforce case creation.
This shows that our integration is stable and tokens are being refreshed properly.â€

â¸»

ğŸ•’ 4. Time Range Controls

â€œThe dashboard allows switching between different time frames â€” like 1 hour, 3 hours, 1 day, etc.
This helps us perform both real-time and historical trend analysis.â€

â¸»

ğŸ“ˆ 5. Insights / Observations

â€œFrom the past dayâ€™s metrics:

	â€¢	All Salesforce case creations have succeeded.
	â€¢	25 TMS events were successfully published.
	â€¢	No TTPublish or CaseCreation failures, indicating system stability.
	â€¢	S3 processing is consistent, showing 6 records processed.â€

â¸»

âš™ï¸ 6. Next Steps / Recommendations

	â€¢	Add alarms for failure metrics (e.g., SFCaseCreationFailure > 0).
	â€¢	Automate metric reporting into Slack or email.
	â€¢	Include latency and processing duration metrics to monitor performance efficiency.

â¸»

ğŸ—£ï¸ 7. Wrap Up

â€œIn summary, this CloudWatch dashboard gives us end-to-end visibility into our BBKYC pipeline health â€” from data ingestion in S3 to case creation in Salesforce.
It helps us ensure timely detection of issues and smooth data flow across multiple AWS services.â€


git fetch origin
git checkout feature/custom-cloudwatch-metrics
git pull origin feature/custom-cloudwatch-metrics
git merge --no-commit --no-ff origin/base-setup

I checked the DynamoDB table â€” the events are getting logged correctly.
I can see multiple records with status = PROCESSED, while a few show status = FAILED with the reason â€œAPI call failedâ€¦â€.
So the Lambdas are processing events, and their statuses are updating properly in the DB.

BBKYC-AUDIT-DATA-PROCESSOR/
â”‚
â”œâ”€â”€ event/
â”‚   â””â”€â”€ input.json
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ main/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”‚
â”‚       â”œâ”€â”€ common/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ common_aws.py
â”‚       â”‚   â””â”€â”€ common.py
â”‚       â”‚
â”‚       â”œâ”€â”€ helper/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ audit.py
â”‚       â”‚   â””â”€â”€ statusHistory.py
â”‚       â”‚
â”‚       â”œâ”€â”€ app.py
â”‚       â””â”€â”€ test/
â”‚           â””â”€â”€ test_app.py
â”‚
â”œâ”€â”€ metadata.yaml
â”œâ”€â”€ product.template.yaml
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ sample-data2.json
â””â”€â”€ setup.sh

Producer lambda changes for parallelization
Shard Distribution Issue
	â€¢	All events going to single shard (Shard 16) despite multiple shards available (7, 11, 14, 16, 17)
	â€¢	Parallelization not taking effect due to poor distribution
	â€¢	Root cause: partition key implementation in producer lambda needs modification
Required Code Changes
	â€¢	Update partition key in BDP Consumer Lambda (rename from â€œBDP Producerâ€)
	â—¦	Change to: existing string + transaction ID
	â—¦	Generates new partition key for each transaction
	â€¢	Convert remaining printf statements to logger statements
	â—¦	Code review needed but will be handled separately by Ria
	â€¢	Focus only on partition key changes for now
Deployment Process
	â€¢	Make changes in feature branch without merging initially
	â€¢	Package and deploy lambda to sandbox environment first
	â€¢	Test functionality before proceeding with merge
	â€¢	After deployment confirmed working:
	1	Take pull from base setup branch
	2	Resolve any merge conflicts carefully
	3	Raise PR/MR request
Technical Issues
	â€¢	Screen glitching problems affecting both participants
	â—¦	Similar issue experienced for 6-7 days previously
	â—¦	Suspected hardware/heat issues with Mac machines
	â€¢	Display problems may impact development timeline
Next Steps
	â€¢	Immediate: Make partition key changes and deploy to sandbox
	â€¢	Priority: Enable 50,000 record processing capability
	â€¢	Later: Complete branch merging and conflict resolution after initial deployment confirmed working

Partitionkey unique key with transaction id force it to publish to new shard so data will distribute all along


#
Metric Name
What It Tracks
Where Itâ€™s Published (Lambda)
Purpose
1
S3 Record Count
Number of files/records successfully read from the S3 bucket (bbkyc-tc-bdp-bucket or similar)
Lambda: bbkyc-tc-event-producer (after reading from S3)
Helps verify that input data ingestion is working and how many records are processed per trigger.
2
TT Event Publish Success
Count of successful â€œTT eventsâ€ pushed into Kinesis Data Stream (bbkyc-tt-event-stream)
Lambda: bbkyc-tt-event-producer
Confirms that event generation and publishing to the stream are functioning as expected.
3
TT Event Publish Failure
Count of failures when publishing TT events to Kinesis
Same Lambda: bbkyc-tt-event-producer (inside catch/error block)
Helps identify reliability issues or connectivity errors with the event stream.
4
Salesforce Case Creation Success
Number of successfully created Salesforce cases (via REST API call)
Lambda: bbkyc-tf-case-creator
Tracks successful case creation in Salesforce â€” validates downstream integration.
5
Salesforce Case Creation Failure
Number of failed Salesforce case creations
Lambda: bbkyc-tf-case-creator (error handling block)
Detects external API errors, token issues, or invalid data causing failures.



Monitoring dashboard setup for AWS Lambda metrics
AWS CloudWatch Dashboard Setup
	â€¢	Lambda trigger process explanation
	â—¦	Upload JSON file to S3 â†’ Lambda triggers â†’ Metrics generated â†’ Pushed to internal metric store
	â—¦	Monitoring dashboard reads metric points independently of alarm triggers
	â—¦	Statistical analysis options: average, summation, P99 percentile calculations
	â€¢	Widget configuration walkthrough
	â—¦	Add multiple metrics to same widget via plus button â†’ Browse â†’ AWS Lambda namespace
	â—¦	Search by function name or resource name
	â—¦	Graph matrices tab shows multiple values
	â—¦	Rename widget labels for clarity (AWS generates random unique names)
Error Handling Strategy
	â€¢	Two approaches for Lambda error tracking
	1	Code fails â†’ Lambda fails â†’ Error propagates to top (captured by AWS Lambda error matrix)
	2	Handle errors in code â†’ Lambda never fails â†’ Generate custom error matrices from code
	â€¢	Review existing producer Lambda error handling
	â—¦	Determine if failures are captured at Lambda error matrix level
	â—¦	Decide between infrastructure-level vs application-level error tracking
	â€¢	Widget naming: Use descriptive titles like â€œTT Publish failureâ€ instead of AWS defaults
Dashboard Enhancement Requirements
	â€¢	Add text sections describing what matrices mean
	â—¦	Business users need context without technical knowledge
	â—¦	Make dashboard self-explanatory for non-developers
	â€¢	Uncheck sparkline option to reduce visual clutter (Edit â†’ Options tab)
	â€¢	Current data shows only single record processing - need more data for proper testing
Next Steps
	â€¢	Ria: Resolve WiFi issues, complete scenario generation
	â€¢	Review and update error handling in producer Lambda
	â€¢	Beautify dashboard with descriptive text sections
	â€¢	Follow up after implementation for CFD review of monitoring dashboard


git filter-branch --force --index-filter "git rm -r --cached --ignore-unmatch venv" --prune-empty --tag-name-filter cat -- --all


tms_token_manager/
â”‚
â”œâ”€â”€ main/
â”‚   â”œâ”€â”€ helpers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ audit_helper.py       # if applicable
â”‚   â”‚   â”œâ”€â”€ kinesis_helper.py     # if applicable
â”‚   â”‚   â”œâ”€â”€ utils.py              # shared functions or secrets logic
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ lambda_handler.py         # entry Lambda file
â”‚
â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ helpers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ utils.py              # mock/test utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ lambda_handler.py         # for testing only (unit test or mocks)
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ template.yaml
â”œâ”€â”€ env-vars.json
â”œâ”€â”€ response.json
â””â”€â”€ README.md

Custom metrics implementation for TT events
CloudWatch Metrics Implementation Requirements
	â€¢	Five custom metrics needed across two Lambda functions:
	â—¦	TT Producer Lambda: S3 records received count, TT events published successfully, TT events failed to publish
	â—¦	Case Creation Lambda: SF case creation success, SF case creation failure
	â€¢	Must implement from code itself, not console
	â—¦	Console-created metrics wonâ€™t show actual data flow
	â—¦	Custom CloudWatch metrics provide real-time success/failure tracking
Current Metrics Issues
	â€¢	Existing AWS metrics insufficient for monitoring:
	â—¦	â€œNumber of objectsâ€ shows file count, not record count within files
	â—¦	â€œInvocationsâ€ shows Lambda execution count, not individual event processing
	â—¦	Example: 1 file with 100 records = 1 invocation but 100 events to track
	â€¢	Need code-level metrics to capture granular success/failure data
Implementation Process
	â€¢	Code changes required in both Lambda functions:
	â—¦	Pull latest code from SAIâ€™s recent structural changes
	â—¦	Create separate branch from base setup to avoid overriding working code
	â—¦	Implement custom CloudWatch metrics within Lambda code
	â€¢	Deployment considerations:
	â—¦	Keep backup of current zip files
	â—¦	Update product template YAML if zip names change
	â—¦	Stack wonâ€™t update, only underlying code changes
Verification & Testing
	â€¢	Metrics automatically appear in CloudWatch after code deployment
	â€¢	Check â€œAll Metricsâ€ â†’ â€œpkyc/Salesforceâ€ namespace for custom metrics
	â€¢	Successful implementation shows graph representation with peaks/valleys
	â€¢	Test with file upload: 5 records should generate 5 case creation success metrics
Next Steps
	â€¢	Pull latest code and create new branch
	â€¢	Implement custom metrics in both Lambda functions
	â€¢	Deploy changes and verify metrics appear in CloudWatch dashboard
	â€¢	Test with sample file upload to confirm tracking accuracy

